{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvWnoZbzqC7u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVAuaBLQqM8e",
        "outputId": "9774f977-d2d6-4292-a637-e4c9dc3b5a63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J-N_Z7Jppog"
      },
      "source": [
        "First Attempt. Was not successful. Information loss and error values were MASSIVE. Decided to come back later, if time provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXQqcZIKBBKu"
      },
      "source": [
        "Did come back later, resized images to 256x256. Loss still remained massive, so was not able to optimize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.905291Z",
          "iopub.status.busy": "2024-10-04T23:04:45.904921Z",
          "iopub.status.idle": "2024-10-04T23:04:45.912278Z",
          "shell.execute_reply": "2024-10-04T23:04:45.911135Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.905201Z"
        },
        "id": "oVOYEUYlppok",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, h_dim=5000, z_dim=500):\n",
        "        super().__init__()\n",
        "        # encoder\n",
        "        self.img_2hid = nn.Linear(input_dim, h_dim)\n",
        "        self.hid_2mu = nn.Linear(h_dim, z_dim)\n",
        "        self.hid_2sigma = nn.Linear(h_dim, z_dim)\n",
        "\n",
        "        # decoder\n",
        "        self.z_2hid = nn.Linear(z_dim, h_dim)\n",
        "        self.hid_2img = nn.Linear(h_dim, input_dim)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.relu(self.img_2hid(x))\n",
        "        mu, sigma = self.hid_2mu(h), self.hid_2sigma(h)\n",
        "        return mu, sigma\n",
        "\n",
        "    def decode(self, z):\n",
        "        h = self.relu(self.z_2hid(z))\n",
        "        return torch.sigmoid(self.hid_2img(h))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, sigma = self.encode(x)\n",
        "        epsilon = torch.randn_like(sigma)\n",
        "        z_new = mu + sigma*epsilon\n",
        "        x_reconstructed = self.decode(z_new)\n",
        "        return x_reconstructed, mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.913738Z",
          "iopub.status.busy": "2024-10-04T23:04:45.913397Z",
          "iopub.status.idle": "2024-10-04T23:04:45.92168Z",
          "shell.execute_reply": "2024-10-04T23:04:45.9208Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.913693Z"
        },
        "id": "AxpUTVvmppol",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "from tqdm import tqdm\n",
        "from torch import nn, optim\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.926045Z",
          "iopub.status.busy": "2024-10-04T23:04:45.924359Z",
          "iopub.status.idle": "2024-10-04T23:04:45.934702Z",
          "shell.execute_reply": "2024-10-04T23:04:45.933728Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.926007Z"
        },
        "id": "acJJLQUsppom",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class RawReferenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []  # List to store tuples of (raw_img_path, reference_img_path, original_size)\n",
        "\n",
        "        raw_dir = os.path.join(root_dir, 'Raw')\n",
        "        reference_dir = os.path.join(root_dir, 'Reference')\n",
        "\n",
        "        raw_images = sorted(os.listdir(raw_dir))\n",
        "        reference_images = sorted(os.listdir(reference_dir))\n",
        "\n",
        "        for raw_img, ref_img in zip(raw_images, reference_images):\n",
        "            raw_img_path = os.path.join(raw_dir, raw_img)\n",
        "            ref_img_path = os.path.join(reference_dir, ref_img)\n",
        "\n",
        "\n",
        "            original_image = Image.open(raw_img_path)\n",
        "            original_size = original_image.size  # (Width, Height)\n",
        "\n",
        "\n",
        "            self.data.append((raw_img_path, ref_img_path, original_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raw_img_path, ref_img_path, original_size = self.data[idx]\n",
        "\n",
        "\n",
        "        raw_img = Image.open(raw_img_path).convert('RGB')  # Convert to RGB to standardize\n",
        "        ref_img = Image.open(ref_img_path).convert('RGB')\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            raw_img = self.transform(raw_img)\n",
        "            ref_img = self.transform(ref_img)\n",
        "\n",
        "\n",
        "        return raw_img, ref_img, original_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.938185Z",
          "iopub.status.busy": "2024-10-04T23:04:45.937412Z",
          "iopub.status.idle": "2024-10-04T23:04:45.94606Z",
          "shell.execute_reply": "2024-10-04T23:04:45.945108Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.93815Z"
        },
        "id": "hYi97Ecbppoo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "root_dir = \"/content/drive/MyDrive/Task 2/Train\"\n",
        "dataset = RawReferenceDataset(root_dir=root_dir, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7k42HzYuZex"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Configuration\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "INPUT_DIM = 256*256*3\n",
        "H_DIM = 5000\n",
        "Z_DIM = 500\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "LR_RATE = 3e-4  # Karpathy constant\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4VK0TZ1Bd3T"
      },
      "source": [
        "Also tried playing with the learning rate, to see if larger learning rates would help, since error values were massive. Did not help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcelWdRWuVK7"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.947696Z",
          "iopub.status.busy": "2024-10-04T23:04:45.947319Z",
          "iopub.status.idle": "2024-10-04T23:04:45.955558Z",
          "shell.execute_reply": "2024-10-04T23:04:45.954638Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.947652Z"
        },
        "id": "x5zBPtOXppoq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model = VariationalAutoEncoder(INPUT_DIM, H_DIM, Z_DIM).to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR_RATE)\n",
        "loss_fn = nn.BCELoss(reduction=\"sum\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.957644Z",
          "iopub.status.busy": "2024-10-04T23:04:45.956975Z",
          "iopub.status.idle": "2024-10-04T23:04:45.964542Z",
          "shell.execute_reply": "2024-10-04T23:04:45.963611Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.957584Z"
        },
        "id": "6dGpflEpppos",
        "outputId": "c80e2c41-d97f-4d3c-88ba-d540888c7974",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11/11 [01:34<00:00,  8.60s/it, loss=1.51e+19]\n",
            " 18%|█▊        | 2/11 [00:24<01:48, 12.00s/it, loss=5.49e+8]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-b95a09bd72d5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m             )\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 10\n",
        "LR_RATE = 3e-5  # Karpathy constant\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "    for i, (raw_imgs, ref_imgs, original_sizes) in loop:\n",
        "        # Flatten the raw images to fit the model input\n",
        "        raw_imgs = raw_imgs.to(DEVICE).view(raw_imgs.size(0), -1)  # (B, C*H*W)\n",
        "\n",
        "        # Forward pass\n",
        "        x_reconstructed, mu, sigma = model(raw_imgs)\n",
        "\n",
        "        # Compare reconstructed images with reference images\n",
        "        # Ensure ref_imgs are also flattened for comparison\n",
        "        ref_imgs = ref_imgs.to(DEVICE).view(ref_imgs.size(0), -1)  # (B, C*H*W)\n",
        "\n",
        "        # Loss calculation\n",
        "        reconstruction_loss = loss_fn(x_reconstructed, ref_imgs)  # Compare with reference images\n",
        "        kl_div = -torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
        "\n",
        "        # Backpropagation\n",
        "        loss = reconstruction_loss + kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.966151Z",
          "iopub.status.busy": "2024-10-04T23:04:45.965735Z",
          "iopub.status.idle": "2024-10-04T23:04:45.977156Z",
          "shell.execute_reply": "2024-10-04T23:04:45.976172Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.966117Z"
        },
        "id": "glN6pYiEppot",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, raw_dir, reference_dir, transform=None):\n",
        "        self.raw_dir = raw_dir\n",
        "        self.reference_dir = reference_dir\n",
        "        self.transform = transform\n",
        "        self.raw_images = os.listdir(raw_dir)\n",
        "        self.reference_images = os.listdir(reference_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load raw image\n",
        "        raw_image_path = os.path.join(self.raw_dir, self.raw_images[idx])\n",
        "        raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "\n",
        "        # Load reference image\n",
        "        reference_image_path = os.path.join(self.reference_dir, self.reference_images[idx])\n",
        "        reference_image = Image.open(reference_image_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            raw_image = self.transform(raw_image)\n",
        "            reference_image = self.transform(reference_image)\n",
        "\n",
        "        return raw_image, reference_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.978485Z",
          "iopub.status.busy": "2024-10-04T23:04:45.978161Z",
          "iopub.status.idle": "2024-10-04T23:04:45.988137Z",
          "shell.execute_reply": "2024-10-04T23:04:45.987225Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.978451Z"
        },
        "id": "Obw2TbZnppou",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256,256)),  \n",
        "    transforms.ToTensor()          \n",
        "])\n",
        "\n",
        "# Load the TEST dataset\n",
        "test_raw_dir = \"/content/drive/MyDrive/Task 2/Test/Raw\"  # Adjust the path accordingly\n",
        "test_reference_dir = \"/content/drive/MyDrive/Task 2/Test/Reference\"  # Adjust the path accordingly\n",
        "test_dataset = TestDataset(test_raw_dir, test_reference_dir, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:45.99967Z",
          "iopub.status.busy": "2024-10-04T23:04:45.999371Z",
          "iopub.status.idle": "2024-10-04T23:04:46.009947Z",
          "shell.execute_reply": "2024-10-04T23:04:46.008919Z",
          "shell.execute_reply.started": "2024-10-04T23:04:45.999632Z"
        },
        "id": "vKGbk3vyppow",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "first_reconstructed_image = reconstructed_images[0]\n",
        "\n",
        "# Move to CPU and convert to NumPy array\n",
        "first_reconstructed_image_np = first_reconstructed_image.detach().cpu().numpy()\n",
        "\n",
        "# If the image is in CHW format, transpose to HWC format for visualization\n",
        "if first_reconstructed_image_np.shape[0] == 1:  # Grayscale image\n",
        "    first_reconstructed_image_np = first_reconstructed_image_np[0]  # Remove channel dimension\n",
        "elif first_reconstructed_image_np.shape[0] == 3:  # RGB image\n",
        "    first_reconstructed_image_np = first_reconstructed_image_np.transpose(1, 2, 0)  # Convert to HWC\n",
        "\n",
        "# Plot the image\n",
        "plt.imshow(first_reconstructed_image_np, cmap='gray' if first_reconstructed_image_np.ndim == 2 else None)\n",
        "plt.axis('off')  # Hide axis\n",
        "plt.title('First Reconstructed Image')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:04:46.01214Z",
          "iopub.status.busy": "2024-10-04T23:04:46.011208Z",
          "iopub.status.idle": "2024-10-04T23:04:46.021169Z",
          "shell.execute_reply": "2024-10-04T23:04:46.020163Z",
          "shell.execute_reply.started": "2024-10-04T23:04:46.012104Z"
        },
        "id": "gWqFRvU_ppow",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Inference and Metric Calculation\n",
        "\n",
        "#Made this block to calculate metrics. Not that it ever was of use :')\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import numpy as np\n",
        "\n",
        "# Function to calculate PSNR\n",
        "def calculate_psnr(original, reconstructed):\n",
        "    mse_value = F.mse_loss(original, reconstructed).item()\n",
        "    if mse_value == 0:\n",
        "        return float('inf')  \n",
        "    max_pixel = 1.0  # Assuming images are normalized to [0, 1]\n",
        "    psnr_value = 20 * np.log10(max_pixel / np.sqrt(mse_value))\n",
        "    return psnr_value\n",
        "\n",
        "# Test the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "psnr_values = []\n",
        "mse_values = []\n",
        "ssim_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for raw_images, reference_images in test_loader:\n",
        "        raw_images = raw_images.to(DEVICE)\n",
        "        reference_images = reference_images.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed_images, _, _ = model(raw_images.view(raw_images.size(0), -1))\n",
        "\n",
        "        # Reshape reconstructed_images to match reference_images shape\n",
        "        reconstructed_images = reconstructed_images.view(reference_images.size())  # Reshape to (B, C, H, W)\n",
        "\n",
        "        # Calculate metrics\n",
        "        psnr_value = calculate_psnr(reference_images, reconstructed_images)\n",
        "        mse_value = F.mse_loss(reference_images, reconstructed_images).item()\n",
        "\n",
        "        # Convert tensors to numpy arrays for SSIM calculation\n",
        "        reference_np = reference_images.cpu().numpy().squeeze(0).transpose(1, 2, 0)\n",
        "        reconstructed_np = reconstructed_images.cpu().numpy().squeeze(0).transpose(1, 2, 0)\n",
        "\n",
        "        ssim_value = ssim(reference_np, reconstructed_np, multichannel=True)\n",
        "\n",
        "        # Append the results\n",
        "        psnr_values.append(psnr_value)\n",
        "        mse_values.append(mse_value)\n",
        "        ssim_values.append(ssim_value)\n",
        "\n",
        "# Average metrics\n",
        "average_psnr = np.mean(psnr_values)\n",
        "average_mse = np.mean(mse_values)\n",
        "average_ssim = np.mean(ssim_values)\n",
        "\n",
        "print(f\"Average PSNR: {average_psnr:.2f} dB\")\n",
        "print(f\"Average MSE: {average_mse:.4f}\")\n",
        "print(f\"Average SSIM: {average_ssim:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HloGUKRpppox"
      },
      "source": [
        "Second attempt. Used a different source as reference for the code.\n",
        "tried to first implement a VAE for the MNIST dataset, to start from base up.\n",
        "However, errors popped up.\n",
        "\n",
        "Decided this task consumed too much time, moved on to next tasks. If time permits, will come back to this again and debug and implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YUw8CnqCRYD"
      },
      "source": [
        "Time did NOT permit. Was not able to debug."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-10-04T23:20:55.106506Z",
          "iopub.status.busy": "2024-10-04T23:20:55.105639Z",
          "iopub.status.idle": "2024-10-04T23:20:55.387411Z",
          "shell.execute_reply": "2024-10-04T23:20:55.386354Z",
          "shell.execute_reply.started": "2024-10-04T23:20:55.10646Z"
        },
        "id": "RNbHeU0rppoy",
        "outputId": "88bd6640-69ca-4ff5-f62d-99fa4cedd0ff",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28)\n",
            "y_train shape: (60000,)\n",
            "x_test shape: (10000, 28, 28)\n",
            "y_test shape: (10000,)\n"
          ]
        }
      ],
      "source": [
        "#Second attempt. Tried a simplified and more straightforward algo and code\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input, Flatten, Dense, Lambda, Reshape\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#This line of code did not work. so had to manually upload\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) =  mnist.load_data()\n",
        "\n",
        "\n",
        "# Check the shape of the data\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:20:59.372495Z",
          "iopub.status.busy": "2024-10-04T23:20:59.372091Z",
          "iopub.status.idle": "2024-10-04T23:20:59.53167Z",
          "shell.execute_reply": "2024-10-04T23:20:59.530456Z",
          "shell.execute_reply.started": "2024-10-04T23:20:59.372457Z"
        },
        "id": "yAKfIt71ppoy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Normalize and reshape ============\n",
        "\n",
        "#Norm.\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Reshape\n",
        "img_width  = x_train.shape[1]\n",
        "img_height = x_train.shape[2]\n",
        "num_channels = 1 #MNIST --> grey scale so 1 channel\n",
        "x_train = x_train.reshape(x_train.shape[0], img_height, img_width, num_channels)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_height, img_width, num_channels)\n",
        "input_shape = (img_height, img_width, num_channels)\n",
        "# ========================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "execution": {
          "iopub.execute_input": "2024-10-04T23:21:02.168509Z",
          "iopub.status.busy": "2024-10-04T23:21:02.167603Z",
          "iopub.status.idle": "2024-10-04T23:21:02.923766Z",
          "shell.execute_reply": "2024-10-04T23:21:02.922702Z",
          "shell.execute_reply.started": "2024-10-04T23:21:02.168464Z"
        },
        "id": "jS1hWreappoz",
        "outputId": "dbee8ac9-844c-4e4c-b452-72c1a6f9aea4",
        "trusted": true
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "too many indices for array: array is 2-dimensional, but 3 were indexed",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-648a434fc7aa>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m221\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m222\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "#View a few images\n",
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "plt.imshow(x_train[42][:,:,0])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(x_train[420][:,:,0])\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.imshow(x_train[4200][:,:,0])\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.imshow(x_train[42000][:,:,0])\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:26:22.134319Z",
          "iopub.status.busy": "2024-10-04T23:26:22.133344Z",
          "iopub.status.idle": "2024-10-04T23:26:22.222806Z",
          "shell.execute_reply": "2024-10-04T23:26:22.221837Z",
          "shell.execute_reply.started": "2024-10-04T23:26:22.134272Z"
        },
        "id": "takq9pqippoz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# BUILD THE MODEL\n",
        "\n",
        "\n",
        "# # Encoder\n",
        "\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "input_img = Input(shape=input_shape, name='encoder_input')\n",
        "x = Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "conv_shape = x.shape #Shape of conv to be provided to decoder\n",
        "#Flatten\n",
        "x = Flatten()(x)\n",
        "x = Dense(32, activation='relu')(x)\n",
        "\n",
        "\n",
        "z_mu = Dense(latent_dim, name='latent_mu')(x)   #Mean values of encoded input\n",
        "z_sigma = Dense(latent_dim, name='latent_sigma')(x)  #Std dev. (variance) of encoded input\n",
        "\n",
        "#REPARAMETERIZATION TRICK\n",
        "# Define sampling function to sample from the distribution\n",
        "# Reparameterize sample based on the process defined by Gunderson and Huang\n",
        "# into the shape of: mu + sigma squared x eps\n",
        "#This is to allow gradient descent to allow for gradient estimation accurately.\n",
        "def sample_z(args):\n",
        "  z_mu, z_sigma = args\n",
        "  eps = K.random_normal(shape=(K.shape(z_mu)[0], K.int_shape(z_mu)[1]))\n",
        "  return z_mu + K.exp(z_sigma / 2) * eps\n",
        "\n",
        "# sample vector from the latent distribution\n",
        "z = Lambda(sample_z, output_shape=(latent_dim, ), name='z')([z_mu, z_sigma])\n",
        "\n",
        "\n",
        "# Define and summarize encoder model.\n",
        "encoder = Model(input_img, [z_mu, z_sigma, z], name='encoder')\n",
        "print(encoder.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:26:47.017465Z",
          "iopub.status.busy": "2024-10-04T23:26:47.016586Z",
          "iopub.status.idle": "2024-10-04T23:26:47.069974Z",
          "shell.execute_reply": "2024-10-04T23:26:47.068971Z",
          "shell.execute_reply.started": "2024-10-04T23:26:47.017416Z"
        },
        "id": "pvEGGJnippo0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "\n",
        "\n",
        "decoder_input = Input(shape=(latent_dim, ), name='decoder_input')\n",
        "\n",
        "\n",
        "x = Dense(conv_shape[1]*conv_shape[2]*conv_shape[3], activation='relu')(decoder_input)\n",
        "\n",
        "x = Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
        "\n",
        "x = Conv2DTranspose(32, 3, padding='same', activation='relu',strides=(2, 2))(x)\n",
        "\n",
        "x = Conv2DTranspose(num_channels, 3, padding='same', activation='sigmoid', name='decoder_output')(x)\n",
        "\n",
        "# Define and summarize decoder model\n",
        "decoder = Model(decoder_input, x, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "# apply the decoder to the latent sample\n",
        "z_decoded = decoder(z)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-04T23:31:13.019343Z",
          "iopub.status.busy": "2024-10-04T23:31:13.018603Z",
          "iopub.status.idle": "2024-10-04T23:31:13.102988Z",
          "shell.execute_reply": "2024-10-04T23:31:13.101669Z",
          "shell.execute_reply.started": "2024-10-04T23:31:13.0193Z"
        },
        "id": "OhIdQbmzppo0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#Define custom loss\n",
        "\n",
        "class CustomLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = keras.layers.Flatten(x)\n",
        "        z_decoded = keras.layers.Flatten(z_decoded)\n",
        "\n",
        "        # Reconstruction loss (as we used sigmoid activation we can use binarycrossentropy)\n",
        "        recon_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "\n",
        "        # KL divergence\n",
        "        kl_loss = -5e-4 * K.mean(1 + z_sigma - K.square(z_mu) - K.exp(z_sigma), axis=-1)\n",
        "        return K.mean(recon_loss + kl_loss)\n",
        "\n",
        "    # add custom loss to the class\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x\n",
        "\n",
        "\n",
        "y = CustomLayer()([input_img, z_decoded])\n",
        "\n",
        "# =================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgaJgLdCppo0"
      },
      "outputs": [],
      "source": [
        "# VAE\n",
        "\n",
        "vae = Model(input_img, y, name='vae')\n",
        "\n",
        "# Compile VAE\n",
        "vae.compile(optimizer='adam', loss=None)\n",
        "vae.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Npdz9AuJppo1"
      },
      "outputs": [],
      "source": [
        "# Train autoencoder\n",
        "vae.fit(x_train, None, epochs = 10, batch_size = 32, validation_split = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E30wEAfGppo1"
      },
      "outputs": [],
      "source": [
        "# Visualize results\n",
        "# =================\n",
        "#Visualize inputs mapped to the Latent space\n",
        "\n",
        "\n",
        "mu, _, _ = encoder.predict(x_test)\n",
        "#Plot dim1 and dim2 for mu\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.scatter(mu[:, 0], mu[:, 1], c=y_test, cmap='brg')\n",
        "plt.xlabel('dim 1')\n",
        "plt.ylabel('dim 2')\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7WLzCFEppo1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Visualize images\n",
        "\n",
        "sample_vector = np.array([[1,-1]])\n",
        "decoded_example = decoder.predict(sample_vector)\n",
        "decoded_example_reshaped = decoded_example.reshape(img_width, img_height)\n",
        "plt.imshow(decoded_example_reshaped)\n",
        "\n",
        "\n",
        "\n",
        "n = 20  # generate 15x15 digits\n",
        "figure = np.zeros((img_width * n, img_height * n, num_channels))\n",
        "\n",
        "grid_x = np.linspace(-5, 5, n)\n",
        "grid_y = np.linspace(-5, 5, n)[::-1]\n",
        "\n",
        "# decoder for each square in the grid\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(img_width, img_height, num_channels)\n",
        "        figure[i * img_width: (i + 1) * img_width,\n",
        "               j * img_height: (j + 1) * img_height] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY_UZ-boppo1"
      },
      "outputs": [],
      "source": [
        "#Reshape for visualization\n",
        "fig_shape = np.shape(figure)\n",
        "figure = figure.reshape((fig_shape[0], fig_shape[1]))\n",
        "\n",
        "plt.imshow(figure, cmap='gnuplot2')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 5813387,
          "sourceId": 9542718,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5818630,
          "sourceId": 9549903,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
